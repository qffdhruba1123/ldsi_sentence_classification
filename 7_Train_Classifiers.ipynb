{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c1a3c-20a5-45f1-8c6a-10d9e8dc7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from spacy.symbols import NORM\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import plotly.express as px\n",
    "import sys \n",
    "import fasttext\n",
    "sys.path.append('./luima_sbd')\n",
    "import luima_sbd.sbd_utils as luima\n",
    "from spacy.language import Language\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393df1f-5a9b-414d-a37e-06c34895e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,file_name,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./figures/for_report/{file_name}.jpg\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15846ef1-3146-47b8-817a-c3b6b08587ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d70df0-99d2-4619-b174-059cd840c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fpath = './ldsi_s2021/ldsi_bva_sentence_corpus_v1.json'\n",
    "data = json.load(open(corpus_fpath))\n",
    "affirmed = open('./ldsi_s2021/affirmed_ids.txt', 'r').read().split(\"\\n\")\n",
    "denied= open('./ldsi_s2021/denied_ids.txt', 'r').read().split(\"\\n\")\n",
    "remanded = open('./ldsi_s2021/remanded_ids.txt', 'r').read().split(\"\\n\")\n",
    "# print(len(affirmed), len(denied), len(remanded))\n",
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}\n",
    "\n",
    "def make_span_data(documents_by_id, types_by_id, annotations):\n",
    "    span_data = []\n",
    "    for a in annotations:\n",
    "        start = a['start']\n",
    "        end = a['end']\n",
    "        document_txt = documents_by_id[a['document']]['plainText']\n",
    "        atype = a['type']\n",
    "        document_name=documents_by_id[a['document']]['name']\n",
    "        if document_name in affirmed:\n",
    "            decision='affirmed'\n",
    "        elif document_name in denied:\n",
    "            decision='denied'\n",
    "        elif document_name in remanded:\n",
    "            decision='remanded'\n",
    "        sd = {'txt': document_txt[start:end],\n",
    "              'document': a['document'],\n",
    "              'type': types_by_id[atype]['name'],\n",
    "              'start': a['start'],\n",
    "              'start_normalized': a['start'] / len(document_txt),\n",
    "              'end': a['end'],\n",
    "              'name': document_name,\n",
    "              'decisions': decision}\n",
    "        span_data.append(sd)\n",
    "    return span_data\n",
    "\n",
    "spans = make_span_data(documents_by_id, types_by_id, annotations)\n",
    "span_labels = [s['type'] for s in spans]\n",
    "span_decisions = [s['decisions'] for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a71a9-a8a0-400c-9f4e-6d5c79d9a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "aff=random.sample(affirmed, 6)\n",
    "den=random.sample(denied, 6)\n",
    "rem=random.sample(remanded, 6)\n",
    "test_affirm, dev_affirm = aff[0:3], aff[3:6] \n",
    "test_denied, dev_denied = den[0:3], den[3:6] \n",
    "test_remanded, dev_remanded = rem[0:3], rem[3:6] \n",
    "\n",
    "test_ids = test_affirm+test_denied+test_remanded\n",
    "dev_ids = dev_affirm+dev_denied+dev_remanded\n",
    "\n",
    "test_spans=[]\n",
    "dev_spans=[]\n",
    "train_spans=[]\n",
    "for s in spans:\n",
    "    if s['name'] in test_ids:\n",
    "        test_spans.append(s)\n",
    "    elif s['name'] in dev_ids:\n",
    "        dev_spans.append(s)\n",
    "    else:\n",
    "        train_spans.append(s)\n",
    "        \n",
    "unique_files=pd.DataFrame(train_spans).name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ac631-94c5-4c0f-b82d-6ee2070b2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spans_txt = [s['txt'] for s in train_spans]\n",
    "test_spans_txt = [s['txt'] for s in test_spans]\n",
    "dev_spans_txt = [s['txt'] for s in dev_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f2530-9cc3-4abc-9ace-ac2c8abcea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model(\"result/wordEmbeddingsModel.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292a3e9-a305-4d28-89da-ebc5623d6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_nearest_neighbors(\"veteran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bed58-864a-49ab-95ba-f59367154c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "\n",
    "def spacy_tokenize(txt):\n",
    "    nlp.disable_pipes('parser')\n",
    "    doc = nlp.pipe(txt, n_process=4)\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        t=tokens[i]\n",
    "        t1=tokens[i]\n",
    "#         print(t.pos_, t.text)\n",
    "        if(i!=len(tokens)-1):\n",
    "            t1=tokens[i+1]\n",
    "        if(t1!=t and t1.pos_=='PART' and re.search(r'\\'', t1.text)):\n",
    "            scrap = t.text+t1.text\n",
    "            scrap = re.sub(r'\\W','',scrap).lower()\n",
    "            clean_tokens.append(scrap)\n",
    "            i=i+1           \n",
    "        elif t.pos_ == 'PUNCT':\n",
    "            pass\n",
    "        elif t.text in ('Vet. App.','Fed. Cir.'):\n",
    "            lem=t.lemma_\n",
    "            lem=lem.lower()\n",
    "            clean_tokens.append(lem)\n",
    "        elif (t.text[0].isalpha()==False and t.is_digit==False):\n",
    "            if(t.is_upper==False):\n",
    "                pass\n",
    "            else:\n",
    "                lem=t.lemma_\n",
    "                lem=lem.lower()\n",
    "                clean_tokens.append(lem)            \n",
    "        elif t.pos_ == 'NUM':\n",
    "            clean_tokens.append(f'<NUM{len(t)}>')\n",
    "        else:\n",
    "            lem=t.lemma_\n",
    "            lem = re.sub(r'\\W','',lem)\n",
    "            lem=lem.lower()\n",
    "            clean_tokens.append(lem)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84ae20-f6ee-49a2-9884-7381adc6fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_add_spacy_tokens(spans):\n",
    "    for s in spans:\n",
    "        tokens = spacy_tokenize(s['txt'])\n",
    "        s['tokens_spacy'] = tokens\n",
    "        s['tokens_number'] = len(tokens)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "spans_add_spacy_tokens(train_spans)\n",
    "spans_add_spacy_tokens(test_spans)\n",
    "spans_add_spacy_tokens(dev_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2ea71-3ba3-449a-8bc0-bca0e6ffcd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459adbb-2f06-467d-845f-5efc7390f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word_vec(spans):\n",
    "    for s in spans:\n",
    "        final_vector= []\n",
    "        sum_vec= np.zeros(100)\n",
    "        if(len(s[\"tokens_spacy\"])!=0):\n",
    "            for word in s['tokens_spacy']:\n",
    "                w_vec = model.get_word_vector(word)\n",
    "                sum_vec=np.add(w_vec,sum_vec)\n",
    "            final_vector=sum_vec/s['tokens_number']\n",
    "            s['word_vec']=final_vector\n",
    "        else:\n",
    "            s['word_vec']=np.zeros(100)\n",
    "\n",
    "add_word_vec(train_spans)\n",
    "add_word_vec(dev_spans)\n",
    "add_word_vec(test_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3daecf1-0abe-4f44-b880-a7667f3eacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(train_spans)\n",
    "df[df.tokens_number==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119deb5-18de-45a7-9a6a-b1f4433d7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean=df.tokens_number.mean()\n",
    "train_std=df.tokens_number.std()\n",
    "print(train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58b846-d4c0-4b9f-81fb-8b4e9ef7a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tfidf_vectorizer = TfidfVectorizer(tokenizer=spacy_tokenize,\n",
    "                                         min_df=3,\n",
    "                                         ngram_range=(1,1))\n",
    "spacy_tfidf_vectorizer = spacy_tfidf_vectorizer.fit(train_spans_txt)\n",
    "\n",
    "tfidf_features_spacy = spacy_tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e514344-b16e-4e5d-9b4b-f7490612a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name=\"spacy_tfidf_vectorizer\"\n",
    "# dump(spacy_tfidf_vectorizer, f'{file_name}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fbcae-1cff-4128-a5a8-6d7c26561186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_spacy = spacy_tfidf_vectorizer.transform(train_spans_txt).toarray()\n",
    "test_tfidf_spacy = spacy_tfidf_vectorizer.transform(test_spans_txt).toarray()\n",
    "dev_tfidf_spacy = spacy_tfidf_vectorizer.transform(dev_spans_txt).toarray()\n",
    "train_spans_labels = np.array([s['type'] for s in train_spans])\n",
    "test_spans_labels = np.array([s['type'] for s in test_spans])\n",
    "dev_spans_labels = np.array([s['type'] for s in dev_spans])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321c705-c236-4f32-b391-6fa8a1bb82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_top_tfidf(train_spans_txt, \n",
    "               train_tfidf_spacy,\n",
    "               tfidf_features_spacy,\n",
    "               random.randint(0, len(train_spans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c6eb7-d654-4655-90fc-a623f355d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_features_by_class(train_tfidf_spacy, \n",
    "                            train_spans_labels,\n",
    "                            tfidf_features_spacy)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5bd95-2d6b-4983-928b-63db6e07a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF BLOCK\n",
    "def make_feature_vectors_and_labels(spans, vectorizer):\n",
    "    # function takes long to execute\n",
    "    # note: we un-sparse the matrix here to be able to manipulate it\n",
    "    tfidf = spacy_tfidf_vectorizer.transform([s['txt'] for s in spans]).toarray()\n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    num_tokens_norm = np.array([((s['tokens_number']-train_mean)/train_std) for s in spans])\n",
    "#     word_embd = np.array([s['word_vec'] for s in spans])\n",
    "    y = np.array([s['type'] for s in spans])\n",
    "    print(tfidf.shape, starts_normalized.shape, num_tokens_norm.shape)\n",
    "#     , word_embd.shape\n",
    "    X = np.concatenate((tfidf, np.expand_dims(starts_normalized, axis=1), np.expand_dims(num_tokens_norm, axis=1)), axis=1)\n",
    "    return X, y\n",
    "\n",
    "# word_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff1adb-ecc0-4fd5-b96f-0a57fda20540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word_EMBD BLOCK\n",
    "def make_feature_vectors_and_labels(spans, vectorizer):\n",
    "    # function takes long to execute\n",
    "    # note: we un-sparse the matrix here to be able to manipulate it\n",
    "#     tfidf = spacy_tfidf_vectorizer.transform([s['txt'] for s in spans]).toarray()\n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    num_tokens_norm = np.array([((s['tokens_number']-train_mean)/train_std) for s in spans])\n",
    "    word_embd = np.array([s['word_vec'] for s in spans])\n",
    "    y = np.array([s['type'] for s in spans])\n",
    "    print(starts_normalized.shape, num_tokens_norm.shape, word_embd.shape)\n",
    "#     tfidf.shape\n",
    "    X = np.concatenate((word_embd, np.expand_dims(starts_normalized, axis=1), np.expand_dims(num_tokens_norm, axis=1)), axis=1)\n",
    "    return X, y\n",
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46920f-3829-4057-b5d6-a1e8e70bc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_X, train_y = make_feature_vectors_and_labels(train_spans, spacy_tfidf_vectorizer)\n",
    "dev_X, dev_y = make_feature_vectors_and_labels(dev_spans, spacy_tfidf_vectorizer)\n",
    "test_X, test_y = make_feature_vectors_and_labels(test_spans, spacy_tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695e723-1c4e-4a0a-b187-7103433df5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{train_X.shape} {train_y.shape}')\n",
    "print(f'{dev_X.shape} {dev_y.shape}')\n",
    "print(f'{test_X.shape} {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f33c73-9e4c-4504-bc61-e94a94b0b42e",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine Classifier (Linear Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73eca7-550e-42c0-9f3f-6a4a24531752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "param_distributions = {'n_estimators': np.random.randint(1, 5),\n",
    "                       'max_depth': np.random.randint(5, 10)}\n",
    "model_ident=\"Linear Support Vector Machine\"\n",
    "clf = LinearSVC(random_state=0, tol=1e-4)\n",
    "%time clf = clf.fit(train_X, train_y)\n",
    "print(f'{model_ident}\\nTRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print(f'{model_ident}\\nDEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55836a80-f755-49eb-b7b4-e42b5b677b43",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136c78b-516d-49ef-8948-8333c4d93c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf = clf.fit(train_X, train_y)\n",
    "model_ident=\"Logistic Regression\"\n",
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('DEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0f6db-b628-4364-be7d-5cf72314c775",
   "metadata": {},
   "source": [
    "# Radial kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8280b-4925-42c4-83ad-84d300a87fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf', random_state=0)\n",
    "%time clf = clf.fit(train_X, train_y)\n",
    "model_ident=\"Radial Kernet SVM\"\n",
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('DEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91911460-4136-4360-b612-7ded67361fb8",
   "metadata": {},
   "source": [
    "# Polynomial kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89041bb0-2bd1-49e0-8623-99f11633c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='sigmoid', random_state=0)\n",
    "clf = clf.fit(train_X, train_y)\n",
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('DEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "model_ident=\"Sigmoid Kernel SVM\"\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c58974-b831-4e3e-9084-15c7327cd7ce",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02fce93-5e33-472d-8891-01c1f487e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=13, random_state=0)\n",
    "%time clf = clf.fit(train_X, train_y)\n",
    "model_ident = \"Decision Trees\"\n",
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('DEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b154ad-7666-4795-9dc4-394094a4803b",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28bfd4c-d2a2-470a-ae85-87d22d65ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import dump, load\n",
    "file_name=\"random_forest\"\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=0, n_estimators=20)\n",
    "%time clf = clf.fit(train_X, train_y)\n",
    "model_ident = \"Random Forests\"\n",
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('DEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()\n",
    "\n",
    "# dump(clf, f'{file_name}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a15a49-ff32-4628-a1d9-494762843552",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d487b-ba6b-455e-a19c-d5a4d900627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model TFIDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf = clf.fit(train_X, train_y)\n",
    "model_ident=\"Logistic Regression on\\nTFIDF Featurizatin\"\n",
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('DEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "print('TEST:\\n'+classification_report(test_spans_labels, clf.predict(test_X)))\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0d456-1ef6-47f5-9009-6b2516331832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model Word_Embd\n",
    "clf = SVC(kernel='rbf', random_state=0)\n",
    "clf = clf.fit(train_X, train_y)\n",
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('DEV:\\n'+classification_report(dev_spans_labels, clf.predict(dev_X)))\n",
    "print('TEST:\\n'+classification_report(test_spans_labels, clf.predict(test_X)))\n",
    "model_ident=\"Radial Kernel SVM on\\n Word Embedding Featurization\"\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X), classes=list(clf.classes_),file_name=model_ident,\n",
    "                      title=f'Confusion matrix for {model_ident}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d18c72-8bfc-4adc-8ea1-7b920f8ffe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_errors(clf, eval_spans, vectorizer, \n",
    "                      select_true_label=None, \n",
    "                      select_pred_label=None):\n",
    "    eval_X, eval_y = make_feature_vectors_and_labels(eval_spans, vectorizer)\n",
    "    eval_spans_txt = [s['txt'] for s in eval_spans]\n",
    "    eval_spans_labels = [s['type'] for s in eval_spans]\n",
    "    pred_y = clf.predict(eval_X)\n",
    "    for i in range(len(eval_spans)):\n",
    "        true_label = eval_spans_labels[i]\n",
    "        pred_label = pred_y[i]\n",
    "        if true_label != pred_label:\n",
    "            if select_true_label and true_label != select_true_label: continue\n",
    "            if select_pred_label and pred_label != select_pred_label: continue\n",
    "            doc_name = documents_by_id[eval_spans[i]['document']]['name']\n",
    "            print('sentence # '+str(i)+' / case '+doc_name+' / @'+str(eval_spans[i]['start']))\n",
    "            print('pred: '+pred_label+' / true: '+true_label)\n",
    "            print(eval_spans[i]['txt'])\n",
    "            print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d02ca7-7417-40e9-8859-59defc7bec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file=\"Radial_Kernel_SVM_WordEmb\"\n",
    "dump(clf, f'{model_file}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc763c93-5a25-4719-bd95-9c2c78489dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_errors(clf,\n",
    "                  random.sample(train_spans, 100),\n",
    "                  spacy_tfidf_vectorizer,\n",
    "                  select_true_label='PolicyBasedReasoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecc947-0de9-4a8e-9cfc-711080641d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_errors(clf,\n",
    "                  random.sample(train_spans, 3000),\n",
    "                  spacy_tfidf_vectorizer,\n",
    "                  select_pred_label='LegalRule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faf44d-1a69-44ac-becb-40b605a2e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_errors(clf,\n",
    "                  random.sample(train_spans, 5000),\n",
    "                  spacy_tfidf_vectorizer,\n",
    "                  select_pred_label='LegalPolicy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9363ba43-8c58-46a0-bcc8-62e9c4421a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_errors(clf,\n",
    "                  random.sample(train_spans, 5000),\n",
    "                  spacy_tfidf_vectorizer,\n",
    "                  select_pred_label='RemandInstructions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88aaf7-d5ff-444a-b291-0a1c371483fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
