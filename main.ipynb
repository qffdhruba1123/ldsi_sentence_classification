{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce2f716-1370-4026-8a53-f97f56702250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from spacy.symbols import NORM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sys \n",
    "import fasttext\n",
    "sys.path.append('./luima_sbd')\n",
    "import luima_sbd.sbd_utils as luima\n",
    "from spacy.language import Language\n",
    "random.seed(42)\n",
    "from joblib import dump, load\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a373f3ab-ba47-4493-b624-bbab7615afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "wv_model = fasttext.load_model(\"wordEmbeddingsModel.bin\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "\n",
    "def spacy_tokenize(txt):\n",
    "    nlp.disable_pipes('parser')\n",
    "    doc = nlp.pipe(txt, n_process=4)\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        t=tokens[i]\n",
    "        t1=tokens[i]\n",
    "#         print(t.pos_, t.text)\n",
    "        if(i!=len(tokens)-1):\n",
    "            t1=tokens[i+1]\n",
    "        if(t1!=t and t1.pos_=='PART' and re.search(r'\\'', t1.text)):\n",
    "            scrap = t.text+t1.text\n",
    "            scrap = re.sub(r'\\W','',scrap).lower()\n",
    "            clean_tokens.append(scrap)\n",
    "            i=i+1           \n",
    "        elif t.pos_ == 'PUNCT':\n",
    "            pass\n",
    "        elif t.text in ('Vet. App.','Fed. Cir.'):\n",
    "            lem=t.lemma_\n",
    "            lem=lem.lower()\n",
    "            clean_tokens.append(lem)\n",
    "        elif (t.text[0].isalpha()==False and t.is_digit==False):\n",
    "            if(t.is_upper==False):\n",
    "                pass\n",
    "            else:\n",
    "                lem=t.lemma_\n",
    "                lem=lem.lower()\n",
    "                clean_tokens.append(lem)            \n",
    "        elif t.pos_ == 'NUM':\n",
    "            clean_tokens.append(f'<NUM{len(t)}>')\n",
    "        else:\n",
    "            lem=t.lemma_\n",
    "            lem = re.sub(r'\\W','',lem)\n",
    "            lem=lem.lower()\n",
    "            clean_tokens.append(lem)\n",
    "    return clean_tokens\n",
    "\n",
    "def spans_add_spacy_tokens(spans):\n",
    "    for s in spans:\n",
    "        tokens = spacy_tokenize(s['txt'])\n",
    "        s['tokens_spacy'] = tokens\n",
    "        s['tokens_number'] = len(tokens)\n",
    "        \n",
    "def add_word_vec(spans):\n",
    "    for s in spans:\n",
    "        final_vector= []\n",
    "        sum_vec= np.zeros(100)\n",
    "        if(len(s[\"tokens_spacy\"])!=0):\n",
    "            for word in s['tokens_spacy']:\n",
    "                w_vec = wv_model.get_word_vector(word)\n",
    "                sum_vec=np.add(w_vec,sum_vec)\n",
    "            final_vector=sum_vec/s['tokens_number']\n",
    "            s['word_vec']=final_vector\n",
    "        else:\n",
    "            s['word_vec']=np.zeros(100)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d192fce3-f44f-455e-9089-b248c1460cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file=\"Radial_Kernel_SVM_WordEmb\"\n",
    "vectorizer_file=\"spacy_tfidf_vectorizer\"\n",
    "clf = load(f'{model_file}.joblib') \n",
    "spacy_tfidf_vectorizer = load(f'{vectorizer_file}.joblib')\n",
    "\n",
    "train_mean = 20.592041800643088 \n",
    "train_std = 15.221984070624842\n",
    "for_predict=[]\n",
    "    \n",
    "def make_dict(sentences, indices, doc_length):\n",
    "    for i in range(len(sentences)):\n",
    "        s={}\n",
    "        s['txt']=sentences[i]\n",
    "        s['start_normalized'] = indices[i][0]/doc_length\n",
    "        for_predict.append(s)        \n",
    "    \n",
    "\n",
    "def make_feature_vectors_and_labels(spans):\n",
    "    # function takes long to execute\n",
    "    # note: we un-sparse the matrix here to be able to manipulate it\n",
    "    \n",
    "#     tfidf = spacy_tfidf_vectorizer.transform([s['txt'] for s in spans]).toarray()\n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    num_tokens_norm = np.array([((s['tokens_number']-train_mean)/train_std) for s in spans])\n",
    "    word_embd = np.array([s['word_vec'] for s in spans])\n",
    "#     y = np.array([s['type'] for s in spans])\n",
    "#     print(tfidf.shape, starts_normalized.shape, num_tokens_norm.shape, word_embd.shape)\n",
    "    X = np.concatenate((word_embd, np.expand_dims(starts_normalized, axis=1), np.expand_dims(num_tokens_norm, axis=1)), axis=1)\n",
    "    return X\n",
    "\n",
    "def analyze(doc):\n",
    "    predictions=[]\n",
    "    doc_length=len(doc)\n",
    "    sentences = luima.text2sentences(doc, offsets=False)\n",
    "    index = luima.text2sentences(doc, offsets=True)\n",
    "    make_dict(sentences, index, doc_length)\n",
    "    spans_add_spacy_tokens(for_predict)\n",
    "    add_word_vec(for_predict)\n",
    "    predict_X=make_feature_vectors_and_labels(for_predict)\n",
    "    for_predict.clear()\n",
    "    labels=clf.predict(predict_X)\n",
    "    for i in range(len(sentences)):\n",
    "        array=[sentences[i],labels[i]]\n",
    "        predictions.append(array)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c48615e-e709-4595-85d3-3de3d17444cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['TEST STRING', 'Header']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string=\"TEST STRING\"\n",
    "analyze(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ce0bc-e0ea-445c-ac43-9a3598b6ea64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
