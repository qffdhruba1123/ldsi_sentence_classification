{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314fd076-7291-42f8-814d-59b6c42ace83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f5636-27b8-4b15-b60a-1c12ceca87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52bbab-0519-4fa4-9ffb-f12c137a0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebcc69-57e9-40e5-aa39-28e2208bf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fpath = './ldsi_s2021/ldsi_bva_sentence_corpus_v1.json'\n",
    "data = json.load(open(corpus_fpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431adbf-3e7d-4ae9-b0c9-a1e8664a61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "affirmed = open('./ldsi_s2021/affirmed_ids.txt', 'r').read().split(\"\\n\")\n",
    "denied= open('./ldsi_s2021/denied_ids.txt', 'r').read().split(\"\\n\")\n",
    "remanded = open('./ldsi_s2021/remanded_ids.txt', 'r').read().split(\"\\n\")\n",
    "print(len(affirmed), len(denied), len(remanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80c701-52e6-4d4e-b90e-e5ef0b111628",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb257d-5f71-449f-832a-fc5f3b91c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = [len(d['plainText']) for d in documents_by_id.values()]\n",
    "plt.hist(doc_lengths, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc710ac-1a44-4f37-aef6-09fb7a5d682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sentences assuming every annotation is a sentence\n",
    "def make_span_data(documents_by_id, types_by_id, annotations):\n",
    "    span_data = []\n",
    "    for a in annotations:\n",
    "        start = a['start']\n",
    "        end = a['end']\n",
    "        document_txt = documents_by_id[a['document']]['plainText']\n",
    "        atype = a['type']\n",
    "        document_name=documents_by_id[a['document']]['name']\n",
    "        if document_name in affirmed:\n",
    "            decision='affirmed'\n",
    "        elif document_name in denied:\n",
    "            decision='denied'\n",
    "        elif document_name in remanded:\n",
    "            decision='remanded'\n",
    "        sd = {'txt': document_txt[start:end],\n",
    "              'document': a['document'],\n",
    "              'type': types_by_id[atype]['name'],\n",
    "              'start': a['start'],\n",
    "              'start_normalized': a['start'] / len(document_txt),\n",
    "              'end': a['end'],\n",
    "              'name': document_name,\n",
    "              'decisions': decision}\n",
    "        span_data.append(sd)\n",
    "    return span_data\n",
    "\n",
    "spans = make_span_data(documents_by_id, types_by_id, annotations)\n",
    "span_labels = [s['type'] for s in spans]\n",
    "span_decisions = [s['decisions'] for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22726ee2-0d23-4e09-b8e4-e935dad6f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "aff=random.sample(affirmed, 6)\n",
    "den=random.sample(denied, 6)\n",
    "rem=random.sample(remanded, 6)\n",
    "test_affirm, dev_affirm = aff[0:3], aff[3:6] \n",
    "test_denied, dev_denied = den[0:3], den[3:6] \n",
    "test_remanded, dev_remanded = rem[0:3], rem[3:6] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9deddf-1dab-4d34-aee1-db581da9c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test_affirm+test_denied+test_remanded\n",
    "dev_ids = dev_affirm+dev_denied+dev_remanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f58547-b83d-483f-8c48-fa8a82faabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spans=[]\n",
    "dev_spans=[]\n",
    "train_spans=[]\n",
    "for s in spans:\n",
    "    if s['name'] in test_ids:\n",
    "        test_spans.append(s)\n",
    "    elif s['name'] in dev_ids:\n",
    "        dev_spans.append(s)\n",
    "    else:\n",
    "        train_spans.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b66c2b-d2a1-461b-8280-9e4f5a975100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spans_txt = [s['txt'] for s in train_spans]\n",
    "test_spans_txt = [s['txt'] for s in test_spans]\n",
    "dev_spans_txt = [s['txt'] for s in dev_spans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0a0c3-9b0c-479a-8f07-e779b3a7d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_files=pd.DataFrame(train_spans).name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40bc8b0-f6f9-436c-9d66-e87771d7e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_tp=0\n",
    "tot_fp=0\n",
    "tot_fn=0\n",
    "result = []\n",
    "for file in unique_files:\n",
    "    print(file)\n",
    "# '1204131.txt'\n",
    "# unique_files[file_index] \n",
    "    train_ann_doc=[]\n",
    "    for span in spans:\n",
    "        if(span['name']==file):\n",
    "            train_ann_doc.append(span)\n",
    "     \n",
    "    doc=[]\n",
    "    for d in data['documents']:\n",
    "        if (d['name']==file):\n",
    "            doc.append(d)\n",
    "    \n",
    "    true_start=[]\n",
    "    true_end=[]\n",
    "    for ann in train_ann_doc:\n",
    "        true_start.append(ann['start'])\n",
    "        true_end.append(ann['end'])\n",
    "        \n",
    "    tot_sent=len(true_start)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    scrap = nlp(doc[0]['plainText'])\n",
    "\n",
    "    assert scrap.has_annotation(\"SENT_START\")\n",
    "\n",
    "    tp_count=0\n",
    "    fn_count=0\n",
    "    fp_count=0\n",
    "    count=0\n",
    "    for sent in scrap.sents:\n",
    "        start = sent.start_char\n",
    "        end = sent.end_char\n",
    "        flag=0\n",
    "        count=count+1\n",
    "        for i in range(len(true_start)):\n",
    "            start_range=true_start[i]-3\n",
    "            end_range=true_end[i]+3\n",
    "            if(start>=true_end[i]):\n",
    "                continue;\n",
    "            if((start>=start_range and start <= start_range+6) and (end >= end_range-6 and end <= end_range)):         \n",
    "                tp_count+=1\n",
    "#                 print(\"===========TRUE POS===========\")\n",
    "#                 print(sent.text)\n",
    "#                 print(sent.start_char, sent.end_char)\n",
    "#                 print(true_start[i],true_end[i])\n",
    "                flag=1\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "    fp_count=count-tp_count    \n",
    "    fn_count=tot_sent-tp_count\n",
    "    print(f\"For File:{file}\\n True Positive:{tp_count}\\n False Positive:{fp_count}\\n False Negative:{fn_count}\\n\")\n",
    "    doc_prec=tp_count/(tp_count+fp_count)\n",
    "    doc_recall=tp_count/(tp_count+fn_count)\n",
    "    doc_f1=2*doc_prec*doc_recall/(doc_prec+doc_recall)\n",
    "    \n",
    "    print(f\"For File: {file} Precision: {doc_prec} Recall: {doc_recall} F1 Score: {doc_f1}\\n\")\n",
    "    \n",
    "    diction={\n",
    "        \"File\": file,\n",
    "        \"Precision\": doc_prec,\n",
    "        \"Recall\": doc_recall,\n",
    "        \"F1_Score\": doc_f1\n",
    "    }\n",
    "    result.append(diction)\n",
    "    \n",
    "    tot_tp=tot_tp+tp_count\n",
    "    tot_fp=tot_fp+fp_count\n",
    "    tot_fn=tot_fn+fn_count\n",
    "\n",
    "print(f\"Total Stats \\n True Positive:{tot_tp}\\n False Positive:{tot_fp}\\n False Negative:{tot_fn}\\n\")\n",
    "\n",
    "\n",
    "prec=tot_tp/(tot_tp+tot_fp)\n",
    "recall=tot_tp/(tot_tp+tot_fn)\n",
    "f1_score=2*prec*recall/(prec+recall)\n",
    "print(f\"Precision: {prec}\\n Recall: {recall}\\n F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb8ab0-ab3b-40b9-82de-2b06b749d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(result, key=lambda k: k['Precision'])\n",
    "result[0:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
