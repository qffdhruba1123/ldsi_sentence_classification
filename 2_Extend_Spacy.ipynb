{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04c16e-5d31-4c69-be32-0b5754f557a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from spacy.language import Language\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd938dd-1f0a-42a2-b105-b2c2b586fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9eb0d-0623-4fc7-889a-91961aaa001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63bc15-1d1d-4547-8074-fb50c810a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fpath = './ldsi_s2021/ldsi_bva_sentence_corpus_v1.json'\n",
    "data = json.load(open(corpus_fpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d1ed0-f748-4bee-b13e-cbe8da8d7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "affirmed = open('./ldsi_s2021/affirmed_ids.txt', 'r').read().split(\"\\n\")\n",
    "denied= open('./ldsi_s2021/denied_ids.txt', 'r').read().split(\"\\n\")\n",
    "remanded = open('./ldsi_s2021/remanded_ids.txt', 'r').read().split(\"\\n\")\n",
    "# print(len(affirmed), len(denied), len(remanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e137b8e-0b9c-45fe-bb2e-dc64d0509f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdaa2a5-d34e-4065-8685-5b4e36012999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sentences assuming every annotation is a sentence\n",
    "def make_span_data(documents_by_id, types_by_id, annotations):\n",
    "    span_data = []\n",
    "    for a in annotations:\n",
    "        start = a['start']\n",
    "        end = a['end']\n",
    "        document_txt = documents_by_id[a['document']]['plainText']\n",
    "        atype = a['type']\n",
    "        document_name=documents_by_id[a['document']]['name']\n",
    "        if document_name in affirmed:\n",
    "            decision='affirmed'\n",
    "        elif document_name in denied:\n",
    "            decision='denied'\n",
    "        elif document_name in remanded:\n",
    "            decision='remanded'\n",
    "        sd = {'txt': document_txt[start:end],\n",
    "              'document': a['document'],\n",
    "              'type': types_by_id[atype]['name'],\n",
    "              'start': a['start'],\n",
    "              'start_normalized': a['start'] / len(document_txt),\n",
    "              'end': a['end'],\n",
    "              'name': document_name,\n",
    "              'decisions': decision}\n",
    "        span_data.append(sd)\n",
    "    return span_data\n",
    "\n",
    "spans = make_span_data(documents_by_id, types_by_id, annotations)\n",
    "span_labels = [s['type'] for s in spans]\n",
    "span_decisions = [s['decisions'] for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cbbf3-fa1b-4e99-9ef4-25fe59f32994",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "aff=random.sample(affirmed, 6)\n",
    "den=random.sample(denied, 6)\n",
    "rem=random.sample(remanded, 6)\n",
    "test_affirm, dev_affirm = aff[0:3], aff[3:6] \n",
    "test_denied, dev_denied = den[0:3], den[3:6] \n",
    "test_remanded, dev_remanded = rem[0:3], rem[3:6] \n",
    "\n",
    "test_ids = test_affirm+test_denied+test_remanded\n",
    "dev_ids = dev_affirm+dev_denied+dev_remanded\n",
    "\n",
    "test_spans=[]\n",
    "dev_spans=[]\n",
    "train_spans=[]\n",
    "for s in spans:\n",
    "    if s['name'] in test_ids:\n",
    "        test_spans.append(s)\n",
    "    elif s['name'] in dev_ids:\n",
    "        dev_spans.append(s)\n",
    "    else:\n",
    "        train_spans.append(s)\n",
    "        \n",
    "unique_files=pd.DataFrame(train_spans).name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e0e99-6c29-4b19-877e-36f5761d2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ids)\n",
    "print(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7b569-1131-47c6-9aa5-76fbd32af9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND SPACY CELL\n",
    "\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text in (\"’s\", \"'s\"):\n",
    "            doc[i].is_sent_start = False\n",
    "        elif token.text in (\"“\", \"‘\") and i < len(doc) - 1:\n",
    "            doc[i+1].is_sent_start = False\n",
    "        elif token.text in (\"”\", \"’\"):\n",
    "            doc[i].is_sent_start = False\n",
    "        elif token.text in (\"\\n\",\"\\t\",\"\\r\",\" \",\"  \",\"   \",\"    \",\"DC.\",\"Archive\", \"NO.\"):\n",
    "            doc[i].is_sent_start = False\n",
    "        elif token.text ==\"DOCKET\":\n",
    "            doc[i].is_sent_start = False\n",
    "        elif token.text in (\"THE\",\"REPRESENTATION\",\"WITNESS\",\"ATTORNEY\",\"REASONS\",\"____________________________________________\",\"ORDER\",\"INTRODUCTION\",\"CONCLUSION\",\"FINDINGS\"):\n",
    "            doc[i].is_sent_start = True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d47c4-c68e-42b7-a43c-882db213b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ann_doc=[]\n",
    "for span in spans:\n",
    "    if(span['name']=='0634451.txt'):\n",
    "        train_ann_doc.append(span)\n",
    "train_ann_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a33844-c3d3-445d-adab-b5fd1aa5d882",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot_tp=0\n",
    "tot_fp=0\n",
    "tot_fn=0\n",
    "result = []\n",
    "for file in unique_files:\n",
    "    print(file)\n",
    "# '1204131.txt'\n",
    "# unique_files[file_index] \n",
    "    train_ann_doc=[]\n",
    "    for span in spans:\n",
    "        if(span['name']==file):\n",
    "            train_ann_doc.append(span)\n",
    "     \n",
    "    doc=[]\n",
    "    for d in data['documents']:\n",
    "        if (d['name']==file):\n",
    "            doc.append(d)\n",
    "    \n",
    "    true_start=[]\n",
    "    true_end=[]\n",
    "    for ann in train_ann_doc:\n",
    "        true_start.append(ann['start'])\n",
    "        true_end.append(ann['end'])\n",
    "        \n",
    "    tot_sent=len(true_start)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "    scrap = nlp(doc[0]['plainText'])\n",
    "\n",
    "    assert scrap.has_annotation(\"SENT_START\")\n",
    "\n",
    "    tp_count=0\n",
    "    fn_count=0\n",
    "    fp_count=0\n",
    "    count=0\n",
    "    for sent in scrap.sents:\n",
    "        start = sent.start_char\n",
    "        end = sent.end_char\n",
    "        flag=0\n",
    "        count=count+1\n",
    "        for i in range(len(true_start)):\n",
    "            start_range=true_start[i]-3\n",
    "            end_range=true_end[i]+3\n",
    "            if(start>=true_end[i]):\n",
    "                continue;\n",
    "            if((start>=start_range and start <= start_range+6) and (end >= end_range-6 and end <= end_range)):         \n",
    "                tp_count+=1\n",
    "#                 print(\"===========TRUE POS===========\")\n",
    "#                 print(sent.text)\n",
    "#                 print(sent.start_char, sent.end_char)\n",
    "#                 print(true_start[i],true_end[i])\n",
    "                flag=1\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "    fp_count=count-tp_count    \n",
    "    fn_count=tot_sent-tp_count\n",
    "    print(f\"For File:{file}\\n True Positive:{tp_count}\\n False Positive:{fp_count}\\n False Negative:{fn_count}\\n\")\n",
    "    doc_prec=tp_count/(tp_count+fp_count)\n",
    "    doc_recall=tp_count/(tp_count+fn_count)\n",
    "    doc_f1=2*doc_prec*doc_recall/(doc_prec+doc_recall)\n",
    "    \n",
    "    print(f\"For File: {file} Precision: {doc_prec} Recall: {doc_recall} F1 Score: {doc_f1}\\n\")\n",
    "    \n",
    "    tot_tp=tot_tp+tp_count\n",
    "    tot_fp=tot_fp+fp_count\n",
    "    tot_fn=tot_fn+fn_count\n",
    "    diction={\n",
    "        \"File\": file,\n",
    "        \"Precision\": doc_prec,\n",
    "        \"Recall\": doc_recall,\n",
    "        \"F1_Score\": doc_f1\n",
    "    }\n",
    "    result.append(diction)\n",
    "print(f\"Total Stats \\n True Positive:{tot_tp}\\n False Positive:{tot_fp}\\n False Negative:{tot_fn}\\n\")\n",
    "\n",
    "\n",
    "prec=tot_tp/(tot_tp+tot_fp)\n",
    "recall=tot_tp/(tot_tp+tot_fn)\n",
    "f1_score=2*prec*recall/(prec+recall)\n",
    "print(f\"Precision: {prec}\\n Recall: {recall}\\n F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3802c8-d318-47e5-afaa-8689f3fa955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[]\n",
    "for d in data['documents']:\n",
    "    if (d['name']=='0634451.txt'):\n",
    "        doc.append(d)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e7fb2-c5b7-4fbd-adcb-7baf478357be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = sorted(result, key=lambda k: k['Precision'])\n",
    "result[0:3]\n",
    "# doc_observe=['0843259.txt','1638605.txt','1222019.txt']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
