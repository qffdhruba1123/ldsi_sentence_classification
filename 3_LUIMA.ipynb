{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b66800-654b-49f0-a1e0-13dbbe190562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys \n",
    "sys.path.append('./luima_sbd')\n",
    "import luima_sbd.sbd_utils as luima\n",
    "from spacy.language import Language\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d0240-a69c-41fa-8850-c3b6f12092d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f84990-7b02-4a2b-9a78-8dcefef75b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944ae06-1ae3-4aa2-a1a1-89b61eb9770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fpath = './ldsi_s2021/ldsi_bva_sentence_corpus_v1.json'\n",
    "data = json.load(open(corpus_fpath))\n",
    "affirmed = open('./ldsi_s2021/affirmed_ids.txt', 'r').read().split(\"\\n\")\n",
    "denied= open('./ldsi_s2021/denied_ids.txt', 'r').read().split(\"\\n\")\n",
    "remanded = open('./ldsi_s2021/remanded_ids.txt', 'r').read().split(\"\\n\")\n",
    "# print(len(affirmed), len(denied), len(remanded))\n",
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}\n",
    "\n",
    "def make_span_data(documents_by_id, types_by_id, annotations):\n",
    "    span_data = []\n",
    "    for a in annotations:\n",
    "        start = a['start']\n",
    "        end = a['end']\n",
    "        document_txt = documents_by_id[a['document']]['plainText']\n",
    "        atype = a['type']\n",
    "        document_name=documents_by_id[a['document']]['name']\n",
    "        if document_name in affirmed:\n",
    "            decision='affirmed'\n",
    "        elif document_name in denied:\n",
    "            decision='denied'\n",
    "        elif document_name in remanded:\n",
    "            decision='remanded'\n",
    "        sd = {'txt': document_txt[start:end],\n",
    "              'document': a['document'],\n",
    "              'type': types_by_id[atype]['name'],\n",
    "              'start': a['start'],\n",
    "              'start_normalized': a['start'] / len(document_txt),\n",
    "              'end': a['end'],\n",
    "              'name': document_name,\n",
    "              'decisions': decision}\n",
    "        span_data.append(sd)\n",
    "    return span_data\n",
    "\n",
    "spans = make_span_data(documents_by_id, types_by_id, annotations)\n",
    "span_labels = [s['type'] for s in spans]\n",
    "span_decisions = [s['decisions'] for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03b3b8-06d0-4432-ba47-a575a67d2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "aff=random.sample(affirmed, 6)\n",
    "den=random.sample(denied, 6)\n",
    "rem=random.sample(remanded, 6)\n",
    "test_affirm, dev_affirm = aff[0:3], aff[3:6] \n",
    "test_denied, dev_denied = den[0:3], den[3:6] \n",
    "test_remanded, dev_remanded = rem[0:3], rem[3:6] \n",
    "\n",
    "test_ids = test_affirm+test_denied+test_remanded\n",
    "dev_ids = dev_affirm+dev_denied+dev_remanded\n",
    "\n",
    "test_spans=[]\n",
    "dev_spans=[]\n",
    "train_spans=[]\n",
    "for s in spans:\n",
    "    if s['name'] in test_ids:\n",
    "        test_spans.append(s)\n",
    "    elif s['name'] in dev_ids:\n",
    "        dev_spans.append(s)\n",
    "    else:\n",
    "        train_spans.append(s)\n",
    "        \n",
    "unique_files=pd.DataFrame(train_spans).name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88086e49-840f-4af7-914f-e62912dbe6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ids)\n",
    "print(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229feb0-b04e-4a5c-8907-c15ee86243c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "luima.text2sentences(\"Hi, I am a person. Please Save Me.\", offsets=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965c495-e236-410a-9034-9cee3a309078",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(luima.text2sentences(\"Hi, I am a person. Please Save Me.\", offsets=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7d2e9-f6aa-4e20-8c20-a8add7ac339d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot_tp=0\n",
    "tot_fp=0\n",
    "tot_fn=0\n",
    "result = []\n",
    "\n",
    "# [{'File': '0843259.txt',\n",
    "#   'Precision': 0.2916666666666667,\n",
    "#   'Recall': 0.5185185185185185,\n",
    "#   'F1_Score': 0.37333333333333335},\n",
    "#  {'File': '0942105.txt',\n",
    "#   'Precision': 0.3767123287671233,\n",
    "#   'Recall': 0.6043956043956044,\n",
    "#   'F1_Score': 0.46413502109704635},\n",
    "#  {'File': '0820506.txt',\n",
    "#   'Precision': 0.40540540540540543,\n",
    "#   'Recall': 0.6521739130434783,\n",
    "#   'F1_Score': 0.5}]\n",
    "\n",
    "\n",
    "for file in unique_files:\n",
    "    print(file)\n",
    "# '1204131.txt'\n",
    "# unique_files[file_index] \n",
    "    train_ann_doc=[]\n",
    "    for span in spans:\n",
    "        if(span['name']==file):\n",
    "            train_ann_doc.append(span)\n",
    "     \n",
    "    doc=[]\n",
    "    for d in data['documents']:\n",
    "        if (d['name']==file):\n",
    "            doc.append(d)\n",
    "    \n",
    "    true_start=[]\n",
    "    true_end=[]\n",
    "    for ann in train_ann_doc:\n",
    "        true_start.append(ann['start'])\n",
    "        true_end.append(ann['end'])\n",
    "        \n",
    "    tot_sent=len(true_start)\n",
    "    \n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "    sentences = luima.text2sentences(doc[0]['plainText'], offsets=False)\n",
    "    index = luima.text2sentences(doc[0]['plainText'], offsets=True)\n",
    "\n",
    "#     assert scrap.has_annotation(\"SENT_START\")\n",
    "\n",
    "    tp_count=0\n",
    "    fn_count=0\n",
    "    fp_count=0\n",
    "    count=0\n",
    "    for i in range(len(sentences)):\n",
    "        start=index[i][0]\n",
    "        end=index[i][1]\n",
    "#         start = sent.start_char\n",
    "#         end = sent.end_char\n",
    "        count=count+1\n",
    "        flag=0\n",
    "        for i in range(len(true_start)):\n",
    "            start_range=true_start[i]-3\n",
    "            end_range=true_end[i]+3\n",
    "            if(start>=true_end[i]):\n",
    "                continue;\n",
    "            if((start>=start_range and start <= start_range+6) and (end >= end_range-6 and end <= end_range)):         \n",
    "                tp_count+=1\n",
    "#                 print(\"===========TRUE POS===========\")\n",
    "#                 print(sentences[i])\n",
    "#                 print(start, end)\n",
    "#                 print(\"True: \",true_start[i],true_end[i])\n",
    "                flag=1\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "    fp_count=count-tp_count        \n",
    "    fn_count=tot_sent-tp_count\n",
    "#     print(f\"For File:{file}\\n True Positive:{tp_count}\\n False Positive:{fp_count}\\n False Negative:{fn_count}\\n\")\n",
    "    doc_prec=tp_count/(tp_count+fp_count)\n",
    "    doc_recall=tp_count/(tp_count+fn_count)\n",
    "    doc_f1=2*doc_prec*doc_recall/(doc_prec+doc_recall)\n",
    "    \n",
    "#     print(f\"For File: {file} Precision: {doc_prec} Recall: {doc_recall} F1 Score: {doc_f1}\\n\")\n",
    "    \n",
    "    tot_tp=tot_tp+tp_count\n",
    "    tot_fp=tot_fp+fp_count\n",
    "    tot_fn=tot_fn+fn_count\n",
    "    diction={\n",
    "        \"File\": file,\n",
    "        \"Precision\": doc_prec,\n",
    "        \"Recall\": doc_recall,\n",
    "        \"F1_Score\": doc_f1\n",
    "    }\n",
    "    result.append(diction)\n",
    "# print(f\"Total Stats \\n True Positive:{tot_tp}\\n False Positive:{tot_fp}\\n False Negative:{tot_fn}\\n\")\n",
    "\n",
    "\n",
    "prec=tot_tp/(tot_tp+tot_fp)\n",
    "recall=tot_tp/(tot_tp+tot_fn)\n",
    "f1_score=2*prec*recall/(prec+recall)\n",
    "print(f\"Precision: {prec}\\n Recall: {recall}\\n F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f0fff-2f14-4a25-ba5f-7225998e0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc=[]\n",
    "# for d in data['documents']:\n",
    "#     if (d['name']=='0634451.txt'):\n",
    "#         doc.append(d)\n",
    "train_ann_doc=[]\n",
    "for span in spans:\n",
    "    if(span['name']=='0843259.txt'):\n",
    "        train_ann_doc.append(span)\n",
    "train_ann_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029665e-9f02-4d34-a4ab-d08a574183f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(result, key=lambda k: k['Precision'])\n",
    "result[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b18c7e-bd67-46d5-9802-993d43f36d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ce49c-2b1f-4ecd-8395-1e08006da5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
