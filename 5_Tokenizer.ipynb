{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed3dd6-e162-4861-aee5-91591787e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from spacy.symbols import NORM\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import plotly.express as px\n",
    "import sys \n",
    "sys.path.append('./luima_sbd')\n",
    "import luima_sbd.sbd_utils as luima\n",
    "from spacy.language import Language\n",
    "random.seed(42)\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16fcc8-8c02-4f12-83bf-b44dd033d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41c4cc-f7a9-421b-ae44-eea215778ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e72ca3-f90e-48e5-b33e-a823173d705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fpath = './ldsi_s2021/ldsi_bva_sentence_corpus_v1.json'\n",
    "data = json.load(open(corpus_fpath))\n",
    "affirmed = open('./ldsi_s2021/affirmed_ids.txt', 'r').read().split(\"\\n\")\n",
    "denied= open('./ldsi_s2021/denied_ids.txt', 'r').read().split(\"\\n\")\n",
    "remanded = open('./ldsi_s2021/remanded_ids.txt', 'r').read().split(\"\\n\")\n",
    "# print(len(affirmed), len(denied), len(remanded))\n",
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}\n",
    "\n",
    "def make_span_data(documents_by_id, types_by_id, annotations):\n",
    "    span_data = []\n",
    "    for a in annotations:\n",
    "        start = a['start']\n",
    "        end = a['end']\n",
    "        document_txt = documents_by_id[a['document']]['plainText']\n",
    "        atype = a['type']\n",
    "        document_name=documents_by_id[a['document']]['name']\n",
    "        if document_name in affirmed:\n",
    "            decision='affirmed'\n",
    "        elif document_name in denied:\n",
    "            decision='denied'\n",
    "        elif document_name in remanded:\n",
    "            decision='remanded'\n",
    "        sd = {'txt': document_txt[start:end],\n",
    "              'document': a['document'],\n",
    "              'type': types_by_id[atype]['name'],\n",
    "              'start': a['start'],\n",
    "              'start_normalized': a['start'] / len(document_txt),\n",
    "              'end': a['end'],\n",
    "              'name': document_name,\n",
    "              'decisions': decision}\n",
    "        span_data.append(sd)\n",
    "    return span_data\n",
    "\n",
    "spans = make_span_data(documents_by_id, types_by_id, annotations)\n",
    "span_labels = [s['type'] for s in spans]\n",
    "span_decisions = [s['decisions'] for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac63d02-4277-4d22-8269-66760d62f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "aff=random.sample(affirmed, 6)\n",
    "den=random.sample(denied, 6)\n",
    "rem=random.sample(remanded, 6)\n",
    "test_affirm, dev_affirm = aff[0:3], aff[3:6] \n",
    "test_denied, dev_denied = den[0:3], den[3:6] \n",
    "test_remanded, dev_remanded = rem[0:3], rem[3:6] \n",
    "\n",
    "test_ids = test_affirm+test_denied+test_remanded\n",
    "dev_ids = dev_affirm+dev_denied+dev_remanded\n",
    "\n",
    "test_spans=[]\n",
    "dev_spans=[]\n",
    "train_spans=[]\n",
    "for s in spans:\n",
    "    if s['name'] in test_ids:\n",
    "        test_spans.append(s)\n",
    "    elif s['name'] in dev_ids:\n",
    "        dev_spans.append(s)\n",
    "    else:\n",
    "        train_spans.append(s)\n",
    "        \n",
    "unique_files=pd.DataFrame(train_spans).name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5837c9-e8c2-4c44-84e6-b9a5d9475b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ids)\n",
    "print(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b2b5b-e2bc-425e-8725-09bc98a2dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b2c29-83f0-4bc5-bcff-3ae34f6c8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b735b-e430-4e59-9722-e85ec495d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(txt):\n",
    "    nlp.disable_pipes('parser')\n",
    "    doc = nlp.pipe(txt, n_process=4)\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        t=tokens[i]\n",
    "        t1=tokens[i]\n",
    "#         print(t.pos_, t.text)\n",
    "        if(i!=len(tokens)-1):\n",
    "            t1=tokens[i+1]\n",
    "        if(t1!=t and t1.pos_=='PART' and re.search(r'\\'', t1.text)):\n",
    "            scrap = t.text+t1.text\n",
    "            scrap = re.sub(r'\\W','',scrap).lower()\n",
    "            clean_tokens.append(scrap)\n",
    "            i=i+1           \n",
    "        elif t.pos_ == 'PUNCT':\n",
    "            pass\n",
    "        elif t.text in ('Vet. App.','Fed. Cir.'):\n",
    "            lem=t.lemma_\n",
    "            lem=lem.lower()\n",
    "            clean_tokens.append(lem)\n",
    "        elif (t.text[0].isalpha()==False and t.is_digit==False):\n",
    "            if(t.is_upper==False):\n",
    "                pass\n",
    "            else:\n",
    "                lem=t.lemma_\n",
    "                lem=lem.lower()\n",
    "                clean_tokens.append(lem)            \n",
    "        elif t.pos_ == 'NUM':\n",
    "            clean_tokens.append(f'<NUM{len(t)}>')\n",
    "        else:\n",
    "            lem=t.lemma_\n",
    "            lem = re.sub(r'\\W','',lem)\n",
    "            lem=lem.lower()\n",
    "            clean_tokens.append(lem)\n",
    "    return clean_tokens\n",
    "\n",
    "def spans_add_spacy_tokens(spans):\n",
    "    for s in spans:\n",
    "        s['tokens_spacy'] = spacy_tokenize(s['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f35a75-eb51-42e3-990e-3763db1cfedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_add_spacy_tokens(train_spans)\n",
    "spans_add_spacy_tokens(test_spans)\n",
    "spans_add_spacy_tokens(dev_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870c3db-2e2c-4213-ae7d-208ed78d0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0600090.txt\n",
    "df_segmented=pd.read_pickle(\"./sentence_segmented_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85342238-b00f-4d35-9415-2efdf8f571f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_basic_1 = 'In sum, as the preponderance of the evidence is against the Veteran\\'s claim, his appeal must be denied.'\n",
    "example_cit_1 = 'Smith v. Gober, 14 Vet. App. 227 (2000), aff\\'d 281 F.3d 1384 (Fed. Cir. 2002); Dela Cruz v. Principi, 15 Vet. App. 143 (2001); see also Quartuccio v. Principi, 16 Vet. App. 183 (2002).'\n",
    "example_rule_1 = '\"To establish a right to compensation for a present disability, a Veteran must show: \"(1) the existence of a present disability; (2) in-service incurrence or aggravation of a disease or injury; and (3) a causal relationship between the present disability and the disease or injury incurred or aggravated during service\"-the so-called \"nexus\" requirement.\"'\n",
    "example_mixed_1 = 'In Dingess v. Nicholson, 19 Vet. App. 473 (2006), the U.S. Court of Appeals for Veterans Claims held that, upon receipt of an application for a service-connection claim, 38 U.S.C.A. � 5103(a) and 38 C.F.R. � 3.159(b) require VA to provide the claimant with notice that a disability rating and an effective date for the award of benefits will be assigned if service connection is awarded. '\n",
    "debug = \"The Veterans Claims Assistance Act of 2000 (VCAA), Pub. L. No. 106-475, 114 Stat. 2096 (Nov. 9, 2000) (codified at 38 U.S.C.A. §§ 5100, 5102, 5103, 5103A, 5106, 5107, and 5126 (West 2002) redefined VA's duty to assist the veteran in the development of a claim.\"\n",
    "print(\"ONE\", spacy_tokenize(example_basic_1))\n",
    "print(\"TWO\", spacy_tokenize(example_cit_1))\n",
    "print(\"THREE\", spacy_tokenize(example_rule_1))\n",
    "print(\"FOUR\", spacy_tokenize(example_mixed_1))\n",
    "print(\"FIVE\", spacy_tokenize(debug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8110c3c-100e-4240-a3d3-126da7205419",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_track=[]\n",
    "def func_tok(file, sentences):\n",
    "    for s in sentences:\n",
    "        tokens = spacy_tokenize(s)\n",
    "        token_dict = {\n",
    "            \"file_name\": file,\n",
    "            \"sentence\": s,\n",
    "            \"tokens\": tokens,\n",
    "            \"token_count\": len(tokens)\n",
    "        }\n",
    "        token_track.append(token_dict)\n",
    "        \n",
    "%time df_segmented.apply(lambda x: func_tok(x.file_name, x.sentences), axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7987adb-5405-4b42-adec-316697449f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(token_track).to_pickle(\"./token_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6713a-42be-472a-88b5-feb35b6f7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_pickle(\"./token_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf6b6d-b3d8-4ad7-ad03-e98f1bfecba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcf16e-e9f2-4667-95a7-2bda79b97ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = shuffle(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7eb9b9-b100-4cf4-9b03-74a5bd68fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('fastTextFinalShuffled.txt', 'a')\n",
    "\n",
    "def stringify(tokens):\n",
    "    string=' '.join(tokens)\n",
    "    string=string+'\\n'\n",
    "    file_object.write(string)\n",
    "\n",
    "%time df_test[df_test.token_count>5].tokens.apply(stringify)\n",
    "\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6081d-8fec-4728-abd7-8581ad9e0e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36b086-7dd6-4b2b-afa2-5670d6ffe5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in df_test.sentence[df_test.sentence.str.contains(\"211.a\")]:\n",
    "        print(\"one\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892671a-0b17-4922-8b2e-377982632d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27772105-5600-49db-b09e-2a696e3d246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_width= 4\n",
    "# here you can choose your rounding method, I've chosen math.ceil\n",
    "nbins = math.ceil((df_test[\"token_count\"].max() - df_test[\"token_count\"].min()) / bin_width)\n",
    "\n",
    "fig = px.histogram(df_test, x=\"token_count\", nbins=nbins,\n",
    "            width=1200, height=600,\n",
    "            labels={ # replaces default labels by column name\n",
    "                \"token_count\": \"Number of Tokens\",\n",
    "            },\n",
    "            template=\"plotly_white\")\n",
    "fig.update_yaxes(showgrid=True)\n",
    "fig.update_layout(title_text=\"Histogram of Number of Tokens in Each Sentence\", title_x=0.5, font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dd41f-f5cc-4d62-bff1-5c5184b8c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.token_count>5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84b389-8902-4e57-a5f3-2c567dfbeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d280e-e4ed-4092-b581-89bd54fb8b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
